"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Transfer Learning –∏ Stable-Baselines3.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç PPO —Å MobileNetV3 feature extractor.

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç 3 —Ä–µ–∂–∏–º–∞ –æ–±—É—á–µ–Ω–∏—è (curriculum learning):
  - "reach": –î–æ—Ç—è–Ω—É—Ç—å—Å—è –¥–æ –æ–±—ä–µ–∫—Ç–∞ (—Å–∞–º–∞—è –ø—Ä–æ—Å—Ç–∞—è –∑–∞–¥–∞—á–∞, –Ω–∞—á–Ω–∏—Ç–µ —Å –Ω–µ—ë!)
  - "grasp": –î–æ—Ç—è–Ω—É—Ç—å—Å—è –∏ —Å—Ö–≤–∞—Ç–∏—Ç—å –æ–±—ä–µ–∫—Ç  
  - "transfer": –°—Ö–≤–∞—Ç–∏—Ç—å –∏ –ø–µ—Ä–µ–Ω–µ—Å—Ç–∏ –æ–±—ä–µ–∫—Ç –∫ —Ü–µ–ª–∏ (–ø–æ–ª–Ω–∞—è –∑–∞–¥–∞—á–∞)
"""
import os
import json
import numpy as np
import torch
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv, VecMonitor
from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, CallbackList, BaseCallback
from stable_baselines3.common.monitor import Monitor
import matplotlib.pyplot as plt
from robot_visual_env import RobotArmEnv
from feature_extractor import CustomCNNFeatureExtractor


def save_model_config(model_path, config):
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ –≤ JSON —Ñ–∞–π–ª
    """
    config_path = model_path.replace('.zip', '_config.json')
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    print(f"‚úì –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {config_path}")
    return config_path


class EarlyStoppingCallback(BaseCallback):
    """
    –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è –µ—Å–ª–∏ –Ω–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –≤ —Ç–µ—á–µ–Ω–∏–µ patience –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π.
    """
    def __init__(self, patience=20, min_improvement=0.01, verbose=1):
        super().__init__(verbose)
        self.patience = patience  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ü–µ–Ω–æ–∫ –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è –¥–æ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
        self.min_improvement = min_improvement  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ
        self.best_mean_reward = -np.inf
        self.no_improvement_count = 0
        
    def _on_step(self) -> bool:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–µ 10000 —à–∞–≥–æ–≤
        if self.n_calls % 10000 == 0 and self.n_calls > 0:
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–∞–≥—Ä–∞–¥—ã –∏–∑ –ª–æ–≥–æ–≤
            if len(self.model.ep_info_buffer) > 0:
                rewards = [ep['r'] for ep in self.model.ep_info_buffer]
                mean_reward = np.mean(rewards)
                
                if mean_reward > self.best_mean_reward + self.min_improvement:
                    self.best_mean_reward = mean_reward
                    self.no_improvement_count = 0
                    if self.verbose > 0:
                        print(f"\nüìà –ù–æ–≤—ã–π –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {mean_reward:.2f}")
                else:
                    self.no_improvement_count += 1
                    if self.verbose > 0:
                        print(f"\n‚è≥ –ù–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è ({self.no_improvement_count}/{self.patience}), "
                              f"–ª—É—á—à–∏–π: {self.best_mean_reward:.2f}, —Ç–µ–∫—É—â–∏–π: {mean_reward:.2f}")
                
                if self.no_improvement_count >= self.patience:
                    if self.verbose > 0:
                        print(f"\nüõë EARLY STOPPING: –Ω–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è {self.patience} —Ä–∞–∑ –ø–æ–¥—Ä—è–¥")
                    return False
        return True


def make_env(rank=0, image_size=84, use_grayscale=False, frame_skip=4, frame_stack=4, task="reach"):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –æ–±–µ—Ä–Ω—É—Ç–æ–π —Å—Ä–µ–¥—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    """
    def _init():
        env = RobotArmEnv(
            use_gui=False,
            image_size=image_size,
            use_grayscale=use_grayscale,
            frame_skip=frame_skip,
            frame_stack=frame_stack,
            task=task
        )
        env = Monitor(env)
        return env
    return _init


def plot_training_results(log_dir, save_path):
    """
    –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –æ–±—É—á–µ–Ω–∏—è
    """
    from stable_baselines3.common.results_plotter import load_results, ts2xy
    
    results = load_results(log_dir)
    
    # –°–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ
    def moving_average(values, window):
        # –£–±–µ–¥–∏—Ç—å—Å—è —á—Ç–æ values —ç—Ç–æ numpy array —Å —á–∏—Å–ª–æ–≤—ã–º —Ç–∏–ø–æ–º
        values = np.array(values, dtype=np.float64)
        weights = np.repeat(1.0, window) / window
        return np.convolve(values, weights, 'valid')
    
    x, y = ts2xy(results, 'timesteps')
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ numpy arrays
    x = np.array(x, dtype=np.float64)
    y = np.array(y, dtype=np.float64)
    
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
    
    # –ì—Ä–∞—Ñ–∏–∫ 1: –ù–∞–≥—Ä–∞–¥–∞
    ax1.plot(x, y, alpha=0.3, label='Raw')
    if len(y) >= 50:
        y_smooth = moving_average(y, 50)
        ax1.plot(x[len(x)-len(y_smooth):], y_smooth, label='Moving Average (50 episodes)')
    ax1.set_xlabel('Timesteps')
    ax1.set_ylabel('Episode Reward')
    ax1.set_title('Training Progress: Episode Reward')
    ax1.legend()
    ax1.grid(True)
    
    # –ì—Ä–∞—Ñ–∏–∫ 2: –î–ª–∏–Ω–∞ —ç–ø–∏–∑–æ–¥–∞
    x_ep, y_ep = ts2xy(results, 'episodes')
    ep_lengths = np.array(results['l'].values, dtype=np.float64)
    
    ax2.plot(range(len(ep_lengths)), ep_lengths, alpha=0.3, label='Raw')
    if len(ep_lengths) >= 50:
        ep_smooth = moving_average(ep_lengths, 50)
        ax2.plot(range(len(ep_lengths)-len(ep_smooth), len(ep_lengths)), ep_smooth, 
                label='Moving Average (50 episodes)')
    ax2.set_xlabel('Episodes')
    ax2.set_ylabel('Episode Length')
    ax2.set_title('Training Progress: Episode Length')
    ax2.legend()
    ax2.grid(True)
    
    plt.tight_layout()
    plt.savefig(save_path)
    plt.close()
    print(f"‚úì –ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {save_path}")


def train():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
    """
    print("=" * 60)
    print("–û–ë–£–ß–ï–ù–ò–ï –í–ò–ó–£–ê–õ–¨–ù–û–ì–û –£–ü–†–ê–í–õ–ï–ù–ò–Ø –†–û–ë–û–¢–û–ú FRANKA PANDA")
    print("=" * 60)
    
    # ========== –ü–ê–†–ê–ú–ï–¢–†–´ –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–ê ==========
    # üî• –ú–ï–ù–Ø–ô–¢–ï –¢–û–õ–¨–ö–û –≠–¢–ò –ü–ê–†–ê–ú–ï–¢–†–´! üî•
    
    # –†–ï–ñ–ò–ú –ó–ê–î–ê–ß–ò (curriculum learning - –Ω–∞—á–∏–Ω–∞–π—Ç–µ —Å –ø—Ä–æ—Å—Ç–æ–π!)
    # "reach"    - –¢–æ–ª—å–∫–æ –¥–æ—Ç—è–Ω—É—Ç—å—Å—è –¥–æ –æ–±—ä–µ–∫—Ç–∞ (–ù–ê–ß–ù–ò–¢–ï –° –≠–¢–û–ì–û!)
    # "grasp"    - –î–æ—Ç—è–Ω—É—Ç—å—Å—è –∏ —Å—Ö–≤–∞—Ç–∏—Ç—å –æ–±—ä–µ–∫—Ç
    # "transfer" - –ü–æ–ª–Ω–∞—è –∑–∞–¥–∞—á–∞: —Å—Ö–≤–∞—Ç–∏—Ç—å –∏ –ø–µ—Ä–µ–Ω–µ—Å—Ç–∏ –∫ —Ü–µ–ª–∏
    TASK = "reach"  # <-- –ú–ï–ù–Ø–ô–¢–ï –ó–î–ï–°–¨!
    
    # –î–û–û–ë–£–ß–ï–ù–ò–ï: –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–µ–¥—ã–¥—É—â—É—é –º–æ–¥–µ–ª—å –∏–ª–∏ –Ω–∞—á–∞—Ç—å —Å –Ω—É–ª—è?
    # None = –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è
    # "RL3/models/reach_rgb84_200k_best.zip" = –¥–æ–æ–±—É—á–µ–Ω–∏–µ —ç—Ç–æ–π –º–æ–¥–µ–ª–∏
    PRETRAINED_MODEL = None  # <-- –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –∏–ª–∏ None
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å—Ä–µ–¥—ã
    IMAGE_SIZE = 84  # 64 (–±—ã—Å—Ç—Ä–æ), 84 (—Å—Ç–∞–Ω–¥–∞—Ä—Ç), 128 (–º–µ–¥–ª–µ–Ω–Ω–æ)
    USE_GRAYSCALE = False  # True = Grayscale, False = RGB
    FRAME_SKIP = 4  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π –¥–µ–π—Å—Ç–≤–∏—è
    FRAME_STACK = 4  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–¥—Ä–æ–≤ –≤ —Å—Ç–µ–∫–µ
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    TOTAL_TIMESTEPS = 200_000  # reach: 100-200K, grasp: 200-300K, transfer: 500K+
    N_ENVS = 4  # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ —Å—Ä–µ–¥—ã
    USE_SUBPROC = True  # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã
    
    # Early stopping (–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞)
    USE_EARLY_STOPPING = True
    PATIENCE = 15  # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ—Å–ª–µ N –ø—Ä–æ–≤–µ—Ä–æ–∫ –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è
    
    # PPO –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    N_STEPS = 1024
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–∫—Ä—É–≥–ª–µ–Ω–∏–µ timesteps
    steps_per_update = N_STEPS * N_ENVS
    actual_timesteps = ((TOTAL_TIMESTEPS + steps_per_update - 1) // steps_per_update) * steps_per_update
    if actual_timesteps != TOTAL_TIMESTEPS:
        print(f"\n‚ö† TOTAL_TIMESTEPS –æ–∫—Ä—É–≥–ª–µ–Ω–æ: {TOTAL_TIMESTEPS:,} ‚Üí {actual_timesteps:,}")
        print(f"  (PPO —Ç—Ä–µ–Ω–∏—Ä—É–µ—Ç –±–ª–æ–∫–∞–º–∏ –ø–æ {steps_per_update:,} —à–∞–≥–æ–≤)")
        TOTAL_TIMESTEPS = actual_timesteps
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–º–µ–Ω–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
    mode = "gray" if USE_GRAYSCALE else "rgb"
    timesteps_k = TOTAL_TIMESTEPS // 1000
    EXPERIMENT_NAME = f"{TASK}_{mode}{IMAGE_SIZE}_{timesteps_k}k"
    
    # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    models_dir = "RL3/models"
    logs_dir = f"RL3/logs/{EXPERIMENT_NAME}"
    tensorboard_dir = f"RL3/tensorboard/{EXPERIMENT_NAME}"
    
    os.makedirs(models_dir, exist_ok=True)
    os.makedirs(logs_dir, exist_ok=True)
    os.makedirs(tensorboard_dir, exist_ok=True)
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    model_config = {
        "experiment_name": EXPERIMENT_NAME,
        "task": TASK,
        "image_size": IMAGE_SIZE,
        "use_grayscale": USE_GRAYSCALE,
        "frame_skip": FRAME_SKIP,
        "frame_stack": FRAME_STACK,
        "total_timesteps": TOTAL_TIMESTEPS,
        "n_envs": N_ENVS,
        "use_subproc": USE_SUBPROC,
        "algorithm": "PPO",
        "feature_extractor": "MobileNetV3-Small"
    }
    
    # –û–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á
    task_descriptions = {
        "reach": "üéØ –ó–ê–î–ê–ß–ê: REACH - –¥–æ—Ç—è–Ω—É—Ç—å—Å—è –¥–æ –æ–±—ä–µ–∫—Ç–∞ (–∫–∞—Å–∞–Ω–∏–µ = —É—Å–ø–µ—Ö)",
        "grasp": "üéØ –ó–ê–î–ê–ß–ê: GRASP - –¥–æ—Ç—è–Ω—É—Ç—å—Å—è –∏ —Å—Ö–≤–∞—Ç–∏—Ç—å –æ–±—ä–µ–∫—Ç",
        "transfer": "üéØ –ó–ê–î–ê–ß–ê: TRANSFER - —Å—Ö–≤–∞—Ç–∏—Ç—å –∏ –ø–µ—Ä–µ–Ω–µ—Å—Ç–∏ –æ–±—ä–µ–∫—Ç –∫ —Ü–µ–ª–∏"
    }
    
    print(f"\nüìä –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢: {EXPERIMENT_NAME}")
    print(f"\n{task_descriptions.get(TASK, TASK)}")
    print(f"\n–ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å—Ä–µ–¥—ã:")
    print(f"  - Image size: {IMAGE_SIZE}x{IMAGE_SIZE}")
    print(f"  - Image mode: {'Grayscale' if USE_GRAYSCALE else 'RGB'}")
    print(f"  - Frame skip: {FRAME_SKIP}")
    print(f"  - Frame stack: {FRAME_STACK}")
    print(f"\n–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:")
    print(f"  - Total timesteps: {TOTAL_TIMESTEPS:,}")
    print(f"  - Parallel environments: {N_ENVS}")
    print(f"  - Early stopping: {'–î–∞ (patience={})'.format(PATIENCE) if USE_EARLY_STOPPING else '–ù–µ—Ç'}")
    print(f"  - Feature extractor: CustomCNN (–æ–±—É—á–∞–µ—Ç—Å—è —Å –Ω—É–ª—è)")
    print(f"  - Algorithm: PPO")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"  - Device: {device}")
    if device == "cuda":
        print(f"    GPU: {torch.cuda.get_device_name(0)}")
        print(f"    VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–π —Å—Ä–µ–¥—ã
    print("\n‚è≥ –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ä–µ–¥—ã...")
    
    if USE_SUBPROC and N_ENVS > 1:
        env = SubprocVecEnv([make_env(i, IMAGE_SIZE, USE_GRAYSCALE, FRAME_SKIP, FRAME_STACK, TASK) for i in range(N_ENVS)])
        print(f"‚úì SubprocVecEnv ({N_ENVS} –ø—Ä–æ—Ü–µ—Å—Å–æ–≤)")
    else:
        env = DummyVecEnv([make_env(i, IMAGE_SIZE, USE_GRAYSCALE, FRAME_SKIP, FRAME_STACK, TASK) for i in range(N_ENVS)])
        print(f"‚úì DummyVecEnv")
    
    env = VecMonitor(env, logs_dir)
    
    # –°—Ä–µ–¥–∞ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    eval_env = DummyVecEnv([make_env(0, IMAGE_SIZE, USE_GRAYSCALE, FRAME_SKIP, FRAME_STACK, TASK)])
    eval_env = VecMonitor(eval_env, logs_dir)
    
    print("‚úì –°—Ä–µ–¥–∞ —Å–æ–∑–¥–∞–Ω–∞")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–ª–∏—Ç–∏–∫–∏ —Å CustomCNN feature extractor
    # CustomCNN –æ–±—É—á–∞–µ—Ç—Å—è —Å –Ω—É–ª—è - –ª—É—á—à–µ –¥–ª—è –ø—Ä–æ—Å—Ç–æ–π –≥—Ä–∞—Ñ–∏–∫–∏ PyBullet
    policy_kwargs = dict(
        features_extractor_class=CustomCNNFeatureExtractor,
        features_extractor_kwargs=dict(features_dim=256),
        net_arch=dict(pi=[256, 128], vf=[256, 128])  # –ú–µ–Ω—å—à–µ, —Ç.–∫. CNN –æ–±—É—á–∞–µ–º–∞—è
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    if PRETRAINED_MODEL and os.path.exists(PRETRAINED_MODEL):
        print(f"\n‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏: {PRETRAINED_MODEL}")
        model = PPO.load(
            PRETRAINED_MODEL,
            env=env,
            device=device,
            tensorboard_log=tensorboard_dir,
            # –ú–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å learning rate –¥–ª—è fine-tuning
            learning_rate=1e-4,  # –ú–µ–Ω—å—à–µ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è
        )
        print(f"‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è")
        print(f"  –†–µ–∂–∏–º: Fine-tuning (learning_rate=1e-4)")
    else:
        if PRETRAINED_MODEL:
            print(f"\n‚ö† –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {PRETRAINED_MODEL}")
            print("  –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è...")
        
        print(f"\n‚è≥ –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏ PPO...")
        model = PPO(
            "MultiInputPolicy",
            env,
            policy_kwargs=policy_kwargs,
            learning_rate=3e-4,
            n_steps=N_STEPS,
            batch_size=512,
            n_epochs=8,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            ent_coef=0.01,
            verbose=1,
            tensorboard_log=tensorboard_dir,
            device=device
        )
        print("‚úì –ù–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞")
    
    print(f"\n–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø–æ–ª–∏—Ç–∏–∫–∏:")
    print(f"  - Features extractor: CustomCNN (–æ–±—É—á–∞–µ–º–∞—è —Å –Ω—É–ª—è)")
    print(f"  - Features dim: 256")
    print(f"  - Policy network: [256, 128]")
    print(f"  - Value network: [256, 128]")
    
    # Callbacks
    checkpoint_callback = CheckpointCallback(
        save_freq=25000,
        save_path=models_dir,
        name_prefix=EXPERIMENT_NAME
    )
    
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=models_dir,
        log_path=logs_dir,
        eval_freq=10000,
        n_eval_episodes=3,
        deterministic=True,
        render=False
    )
    
    # Wrapper –¥–ª—è –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è best_model –≤ –Ω—É–∂–Ω–æ–µ –∏–º—è
    
    class BestModelRenameCallback(BaseCallback):
        def __init__(self, eval_callback, experiment_name, models_dir, model_config, verbose=0):
            super().__init__(verbose)
            self.eval_callback = eval_callback
            self.experiment_name = experiment_name
            self.models_dir = models_dir
            self.model_config = model_config
            self.last_mean_reward = -np.inf
            
        def _on_step(self) -> bool:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –æ–±–Ω–æ–≤–∏–ª–∞—Å—å –ª–∏ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å
            if hasattr(self.eval_callback, 'best_mean_reward'):
                if self.eval_callback.best_mean_reward > self.last_mean_reward:
                    self.last_mean_reward = self.eval_callback.best_mean_reward
                    # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º best_model.zip –≤ –Ω–∞—à–µ –∏–º—è
                    default_path = os.path.join(self.models_dir, "best_model.zip")
                    new_path = os.path.join(self.models_dir, f"{self.experiment_name}_best.zip")
                    if os.path.exists(default_path):
                        if os.path.exists(new_path):
                            os.remove(new_path)
                        os.rename(default_path, new_path)
                        save_model_config(new_path, self.model_config)
                        if self.verbose > 0:
                            print(f"‚úì –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {new_path}")
            return True
    
    rename_callback = BestModelRenameCallback(eval_callback, EXPERIMENT_NAME, models_dir, model_config)
    
    # –°–ø–∏—Å–æ–∫ callbacks
    callbacks = [checkpoint_callback, eval_callback, rename_callback]
    
    # –î–æ–±–∞–≤–ª—è–µ–º early stopping –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω
    if USE_EARLY_STOPPING:
        early_stopping = EarlyStoppingCallback(patience=PATIENCE, min_improvement=1.0, verbose=1)
        callbacks.append(early_stopping)
    
    callback_list = CallbackList(callbacks)
    
    # –û–±—É—á–µ–Ω–∏–µ
    print("\n" + "=" * 60)
    print("–ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø")
    print("=" * 60)
    print(f"–õ–æ–≥–∏ TensorBoard: tensorboard --logdir={tensorboard_dir}")
    if USE_EARLY_STOPPING:
        print(f"Early stopping: –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ—Å–ª–µ {PATIENCE} –ø—Ä–æ–≤–µ—Ä–æ–∫ –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è")
    print("=" * 60 + "\n")
    
    try:
        model.learn(
            total_timesteps=TOTAL_TIMESTEPS,
            callback=callback_list,
            progress_bar=True
        )
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
        final_model_path = os.path.join(models_dir, f"{EXPERIMENT_NAME}_final.zip")
        model.save(final_model_path)
        save_model_config(final_model_path, model_config)
        print(f"\n‚úì –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {final_model_path}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è best_model –µ—Å–ª–∏ –æ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        best_model_path = os.path.join(models_dir, f"{EXPERIMENT_NAME}_best.zip")
        if os.path.exists(best_model_path):
            save_model_config(best_model_path, model_config)
        print(f"\n‚úì –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {final_model_path}")
        
        # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤
        print("\n‚è≥ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –æ–±—É—á–µ–Ω–∏—è...")
        plot_path = f"./RL3/{EXPERIMENT_NAME}_training.png"
        plot_training_results(logs_dir, plot_path)
        
        print("\n" + "=" * 60)
        print("–û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!")
        print("=" * 60)
        print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤:")
        print(f"  - –ú–æ–¥–µ–ª–∏: {models_dir}")
        print(f"  - –õ–æ–≥–∏: {logs_dir}")
        print(f"  - TensorBoard: {tensorboard_dir}")
        print(f"  - –ì—Ä–∞—Ñ–∏–∫: ./RL3/training_results.png")
        
        print("\n–î–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –ª–æ–≥–æ–≤ TensorBoard –≤—ã–ø–æ–ª–Ω–∏—Ç–µ:")
        print(f"  tensorboard --logdir={tensorboard_dir}")
        
        print("\n–î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ:")
        print("  python ./RL3/test_trained_model.py")
        
    except KeyboardInterrupt:
        print("\n‚ö† –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        interrupted_path = os.path.join(models_dir, f"{EXPERIMENT_NAME}_interrupted.zip")
        model.save(interrupted_path)
        save_model_config(interrupted_path, model_config)
        print("‚úì –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
    
    finally:
        env.close()
        eval_env.close()


if __name__ == "__main__":
    # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    import random
    random.seed(42)
    np.random.seed(42)
    torch.manual_seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(42)
    
    train()
