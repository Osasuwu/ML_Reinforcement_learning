"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Transfer Learning –∏ Stable-Baselines3.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç PPO —Å MobileNetV3 feature extractor.
"""
import os
import json
import numpy as np
import torch
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv, VecMonitor
from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, CallbackList
from stable_baselines3.common.monitor import Monitor
import matplotlib.pyplot as plt
from robot_visual_env import RobotArmEnv
from feature_extractor import MobileNetFeatureExtractor


def save_model_config(model_path, config):
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ –≤ JSON —Ñ–∞–π–ª
    """
    config_path = model_path.replace('.zip', '_config.json')
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    print(f"‚úì –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {config_path}")
    return config_path


def make_env(rank=0, image_size=84, use_grayscale=False, frame_skip=4, frame_stack=4):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –æ–±–µ—Ä–Ω—É—Ç–æ–π —Å—Ä–µ–¥—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    """
    def _init():
        env = RobotArmEnv(
            use_gui=False,
            image_size=image_size,
            use_grayscale=use_grayscale,
            frame_skip=frame_skip,
            frame_stack=frame_stack
        )
        env = Monitor(env)
        return env
    return _init


def plot_training_results(log_dir, save_path):
    """
    –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –æ–±—É—á–µ–Ω–∏—è
    """
    from stable_baselines3.common.results_plotter import load_results, ts2xy
    
    results = load_results(log_dir)
    
    # –°–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ
    def moving_average(values, window):
        # –£–±–µ–¥–∏—Ç—å—Å—è —á—Ç–æ values —ç—Ç–æ numpy array —Å —á–∏—Å–ª–æ–≤—ã–º —Ç–∏–ø–æ–º
        values = np.array(values, dtype=np.float64)
        weights = np.repeat(1.0, window) / window
        return np.convolve(values, weights, 'valid')
    
    x, y = ts2xy(results, 'timesteps')
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ numpy arrays
    x = np.array(x, dtype=np.float64)
    y = np.array(y, dtype=np.float64)
    
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
    
    # –ì—Ä–∞—Ñ–∏–∫ 1: –ù–∞–≥—Ä–∞–¥–∞
    ax1.plot(x, y, alpha=0.3, label='Raw')
    if len(y) >= 50:
        y_smooth = moving_average(y, 50)
        ax1.plot(x[len(x)-len(y_smooth):], y_smooth, label='Moving Average (50 episodes)')
    ax1.set_xlabel('Timesteps')
    ax1.set_ylabel('Episode Reward')
    ax1.set_title('Training Progress: Episode Reward')
    ax1.legend()
    ax1.grid(True)
    
    # –ì—Ä–∞—Ñ–∏–∫ 2: –î–ª–∏–Ω–∞ —ç–ø–∏–∑–æ–¥–∞
    x_ep, y_ep = ts2xy(results, 'episodes')
    ep_lengths = np.array(results['l'].values, dtype=np.float64)
    
    ax2.plot(range(len(ep_lengths)), ep_lengths, alpha=0.3, label='Raw')
    if len(ep_lengths) >= 50:
        ep_smooth = moving_average(ep_lengths, 50)
        ax2.plot(range(len(ep_lengths)-len(ep_smooth), len(ep_lengths)), ep_smooth, 
                label='Moving Average (50 episodes)')
    ax2.set_xlabel('Episodes')
    ax2.set_ylabel('Episode Length')
    ax2.set_title('Training Progress: Episode Length')
    ax2.legend()
    ax2.grid(True)
    
    plt.tight_layout()
    plt.savefig(save_path)
    plt.close()
    print(f"‚úì –ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {save_path}")


def train():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
    """
    print("=" * 60)
    print("–û–ë–£–ß–ï–ù–ò–ï –í–ò–ó–£–ê–õ–¨–ù–û–ì–û –£–ü–†–ê–í–õ–ï–ù–ò–Ø –†–û–ë–û–¢–û–ú FRANKA PANDA")
    print("=" * 60)
    
    # ========== –ü–ê–†–ê–ú–ï–¢–†–´ –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–ê ==========
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å—Ä–µ–¥—ã
    IMAGE_SIZE = 84  # –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: 64 (–±—ã—Å—Ç—Ä–æ), 84 (—Å—Ç–∞–Ω–¥–∞—Ä—Ç), 128 (–º–µ–¥–ª–µ–Ω–Ω–æ)
    USE_GRAYSCALE = False  # True = Grayscale (–±—ã—Å—Ç—Ä–µ–µ, –º–µ–Ω—å—à–µ VRAM), False = RGB (–ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ)
    FRAME_SKIP = 4  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π –¥–µ–π—Å—Ç–≤–∏—è (2, 4, 8)
    FRAME_STACK = 4  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –∫–∞–¥—Ä–æ–≤ –¥–ª—è —Å—Ç–µ–∫–∏–Ω–≥–∞ (–æ–±—ã—á–Ω–æ 4)
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    # 50K - —Ç–æ–ª—å–∫–æ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏, —á—Ç–æ –≤—Å—ë —Ä–∞–±–æ—Ç–∞–µ—Ç
    TOTAL_TIMESTEPS = 500_000  # 50K = —Ç–µ—Å—Ç (10% —É—Å–ø–µ—Ö), 500K = –Ω–æ—Ä–º–∞–ª—å–Ω–æ (50-70%), 1M+ = –æ—Ç–ª–∏—á–Ω–æ (80%+)
    N_ENVS = 8  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö —Å—Ä–µ–¥ (4-8, –±–æ–ª—å—à–µ = –±—ã—Å—Ç—Ä–µ–µ, –Ω–æ –±–æ–ª—å—à–µ RAM)
    USE_SUBPROC = True  # True = –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã (–í–°–ï–ì–î–ê –æ—Å—Ç–∞–≤–ª—è–π—Ç–µ True –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏!)
    
    # PPO –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–≤–ª–∏—è—é—Ç –Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç—å TOTAL_TIMESTEPS)
    N_STEPS = 1024  # –®–∞–≥–æ–≤ –Ω–∞ —Å—Ä–µ–¥—É –ø–µ—Ä–µ–¥ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º (—É–º–µ–Ω—å—à–∏—Ç–µ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è timesteps)
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–∫—Ä—É–≥–ª–µ–Ω–∏–µ TOTAL_TIMESTEPS –¥–æ –∫—Ä–∞—Ç–Ω–æ–≥–æ n_steps * n_envs
    # (PPO —Ç—Ä–µ–Ω–∏—Ä—É–µ—Ç –±–ª–æ–∫–∞–º–∏, –ø–æ—ç—Ç–æ–º—É —Ä–µ–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —à–∞–≥–æ–≤ –±—É–¥–µ—Ç –∫—Ä–∞—Ç–Ω—ã–º —ç—Ç–æ–º—É –∑–Ω–∞—á–µ–Ω–∏—é)
    steps_per_update = N_STEPS * N_ENVS
    actual_timesteps = ((TOTAL_TIMESTEPS + steps_per_update - 1) // steps_per_update) * steps_per_update
    if actual_timesteps != TOTAL_TIMESTEPS:
        print(f"\n‚ö† TOTAL_TIMESTEPS –æ–∫—Ä—É–≥–ª–µ–Ω–æ: {TOTAL_TIMESTEPS:,} ‚Üí {actual_timesteps:,}")
        print(f"  (PPO —Ç—Ä–µ–Ω–∏—Ä—É–µ—Ç –±–ª–æ–∫–∞–º–∏ –ø–æ {steps_per_update:,} —à–∞–≥–æ–≤)")
        TOTAL_TIMESTEPS = actual_timesteps
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–º–µ–Ω–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    mode = "gray" if USE_GRAYSCALE else "rgb"
    timesteps_k = TOTAL_TIMESTEPS // 1000
    EXPERIMENT_NAME = f"{mode}{IMAGE_SIZE}_skip{FRAME_SKIP}_env{N_ENVS}_{timesteps_k}k"
    
    # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ (–æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω—ã –ø–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–º)
    models_dir = ".\\RL3\\models"
    logs_dir = f".\\RL3\\logs\\{EXPERIMENT_NAME}"
    tensorboard_dir = f".\\RL3\\tensorboard\\{EXPERIMENT_NAME}"
    
    os.makedirs(models_dir, exist_ok=True)
    os.makedirs(logs_dir, exist_ok=True)
    os.makedirs(tensorboard_dir, exist_ok=True)
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    model_config = {
        "experiment_name": EXPERIMENT_NAME,
        "task": "pick_and_place",  # –ó–∞–¥–∞—á–∞ –ø–µ—Ä–µ–Ω–æ—Å–∞ –æ–±—ä–µ–∫—Ç–∞
        "image_size": IMAGE_SIZE,
        "use_grayscale": USE_GRAYSCALE,
        "frame_skip": FRAME_SKIP,
        "frame_stack": FRAME_STACK,
        "total_timesteps": TOTAL_TIMESTEPS,
        "n_envs": N_ENVS,
        "use_subproc": USE_SUBPROC,
        "algorithm": "PPO",
        "feature_extractor": "MobileNetV3-Small",
        "action_space": "4D (dx, dy, dz, gripper)"
    }
    
    print(f"\nüìä –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢: {EXPERIMENT_NAME}")
    print(f"\nüéØ –ó–ê–î–ê–ß–ê: –ü–µ—Ä–µ–Ω–æ—Å –æ–±—ä–µ–∫—Ç–∞ –≤ —Ü–µ–ª–µ–≤—É—é —Ç–æ—á–∫—É")
    print(f"   –§–∞–∑–∞ 1: –ü–æ–¥–æ–π—Ç–∏ –∏ —Å—Ö–≤–∞—Ç–∏—Ç—å –æ–±—ä–µ–∫—Ç (–∫—Ä–∞—Å–Ω—ã–π –∫—É–±)")
    print(f"   –§–∞–∑–∞ 2: –ü–µ—Ä–µ–Ω–µ—Å—Ç–∏ –æ–±—ä–µ–∫—Ç –∫ —Ü–µ–ª–∏ (–∑–µ–ª—ë–Ω—ã–π –º–∞—Ä–∫–µ—Ä)")
    print(f"\n–ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å—Ä–µ–¥—ã:")
    print(f"  - Image size: {IMAGE_SIZE}x{IMAGE_SIZE}")
    print(f"  - Image mode: {'Grayscale (1 –∫–∞–Ω–∞–ª)' if USE_GRAYSCALE else 'RGB (3 –∫–∞–Ω–∞–ª–∞)'}")
    print(f"  - Frame skip: {FRAME_SKIP}")
    print(f"  - Frame stack: {FRAME_STACK}")
    print(f"  - Action space: 4D (dx, dy, dz, gripper)")
    print(f"\n–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:")
    print(f"  - Total timesteps: {TOTAL_TIMESTEPS:,}")
    print(f"  - Parallel environments: {N_ENVS}")
    print(f"  - Vectorization: {'SubprocVecEnv (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ)' if USE_SUBPROC else 'DummyVecEnv (–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ)'}")
    print(f"  - Feature extractor: MobileNetV3-Small (Transfer Learning)")
    print(f"  - Algorithm: PPO")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"  - Device: {device}")
    if device == "cuda":
        print(f"    GPU: {torch.cuda.get_device_name(0)}")
        print(f"    VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–π —Å—Ä–µ–¥—ã
    print("\n‚è≥ –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ä–µ–¥—ã...")
    
    if USE_SUBPROC and N_ENVS > 1:
        # SubprocVecEnv - –Ω–∞—Å—Ç–æ—è—â–∏–π –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º (–∫–∞–∂–¥–∞—è —Å—Ä–µ–¥–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ)
        env = SubprocVecEnv([make_env(i, IMAGE_SIZE, USE_GRAYSCALE, FRAME_SKIP, FRAME_STACK) for i in range(N_ENVS)])
        print(f"‚úì –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è SubprocVecEnv ({N_ENVS} –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤)")
    else:
        # DummyVecEnv - –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ –∏–ª–∏ N_ENVS=1)
        env = DummyVecEnv([make_env(i, IMAGE_SIZE, USE_GRAYSCALE, FRAME_SKIP, FRAME_STACK) for i in range(N_ENVS)])
        print(f"‚úì –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è DummyVecEnv (–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ)")
    
    env = VecMonitor(env, logs_dir)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ä–µ–¥—ã –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–≤—Å–µ–≥–¥–∞ DummyVecEnv - –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)
    eval_env = DummyVecEnv([make_env(0, IMAGE_SIZE, USE_GRAYSCALE, FRAME_SKIP, FRAME_STACK)])
    eval_env = VecMonitor(eval_env, logs_dir)
    
    print("‚úì –°—Ä–µ–¥–∞ —Å–æ–∑–¥–∞–Ω–∞")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–ª–∏—Ç–∏–∫–∏ —Å MobileNet feature extractor
    print("\n‚è≥ –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ PPO —Å MobileNetV3 feature extractor...")
    
    policy_kwargs = dict(
        features_extractor_class=MobileNetFeatureExtractor,
        features_extractor_kwargs=dict(features_dim=512),  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 256
        net_arch=dict(pi=[512, 256], vf=[512, 256])  # –£–≤–µ–ª–∏—á–µ–Ω—ã —Å–ª–æ–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ GPU
    )
    
    model = PPO(
        "MultiInputPolicy",
        env,
        policy_kwargs=policy_kwargs,
        learning_rate=3e-4,
        n_steps=N_STEPS,  # –®–∞–≥–æ–≤ –Ω–∞ —Å—Ä–µ–¥—É –ø–µ—Ä–µ–¥ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º
        batch_size=512,  # –ë–æ–ª—å—à–æ–π batch –¥–ª—è GPU
        n_epochs=8,  # –ë–æ–ª—å—à–µ —ç–ø–æ—Ö –¥–ª—è –ª—É—á—à–µ–π –∑–∞–≥—Ä—É–∑–∫–∏ GPU
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        ent_coef=0.01,  # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —ç–Ω—Ç—Ä–æ–ø–∏–∏ –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
        verbose=1,
        tensorboard_log=tensorboard_dir,
        device=device
    )
    
    print("‚úì –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞")
    print(f"\n–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø–æ–ª–∏—Ç–∏–∫–∏:")
    print(f"  - Features extractor: MobileNetV3-Small (–ø—Ä–µ–¥–æ–±—É—á–µ–Ω –Ω–∞ ImageNet)")
    print(f"  - Features dim: 256")
    print(f"  - Policy network: [256, 128]")
    print(f"  - Value network: [256, 128]")
    
    # Callbacks (—É–ø—Ä–æ—â–µ–Ω—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
    checkpoint_callback = CheckpointCallback(
        save_freq=25000,  # –†–µ–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º
        save_path=models_dir,
        name_prefix=EXPERIMENT_NAME
    )
    
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=models_dir,
        log_path=logs_dir,
        eval_freq=10000,  # –†–µ–∂–µ –æ—Ü–µ–Ω–∏–≤–∞–µ–º
        n_eval_episodes=3,  # –ú–µ–Ω—å—à–µ —ç–ø–∏–∑–æ–¥–æ–≤ –æ—Ü–µ–Ω–∫–∏
        deterministic=True,
        render=False
    )
    
    # Wrapper –¥–ª—è –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è best_model –≤ –Ω—É–∂–Ω–æ–µ –∏–º—è
    from stable_baselines3.common.callbacks import BaseCallback
    
    class BestModelRenameCallback(BaseCallback):
        def __init__(self, eval_callback, experiment_name, models_dir, model_config, verbose=0):
            super().__init__(verbose)
            self.eval_callback = eval_callback
            self.experiment_name = experiment_name
            self.models_dir = models_dir
            self.model_config = model_config
            self.last_mean_reward = -np.inf
            
        def _on_step(self) -> bool:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –æ–±–Ω–æ–≤–∏–ª–∞—Å—å –ª–∏ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å
            if hasattr(self.eval_callback, 'best_mean_reward'):
                if self.eval_callback.best_mean_reward > self.last_mean_reward:
                    self.last_mean_reward = self.eval_callback.best_mean_reward
                    # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º best_model.zip –≤ –Ω–∞—à–µ –∏–º—è
                    default_path = os.path.join(self.models_dir, "best_model.zip")
                    new_path = os.path.join(self.models_dir, f"{self.experiment_name}_best.zip")
                    if os.path.exists(default_path):
                        if os.path.exists(new_path):
                            os.remove(new_path)
                        os.rename(default_path, new_path)
                        save_model_config(new_path, self.model_config)
                        if self.verbose > 0:
                            print(f"‚úì –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {new_path}")
            return True
    
    rename_callback = BestModelRenameCallback(eval_callback, EXPERIMENT_NAME, models_dir, model_config)
    
    callback_list = CallbackList([checkpoint_callback, eval_callback, rename_callback])
    
    # –û–±—É—á–µ–Ω–∏–µ
    print("\n" + "=" * 60)
    print("–ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø")
    print("=" * 60)
    print(f"–õ–æ–≥–∏ TensorBoard: tensorboard --logdir={tensorboard_dir}")
    print("=" * 60 + "\n")
    
    try:
        model.learn(
            total_timesteps=TOTAL_TIMESTEPS,
            callback=callback_list,
            progress_bar=True
        )
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
        final_model_path = os.path.join(models_dir, f"{EXPERIMENT_NAME}_final.zip")
        model.save(final_model_path)
        save_model_config(final_model_path, model_config)
        print(f"\n‚úì –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {final_model_path}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è best_model –µ—Å–ª–∏ –æ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        best_model_path = os.path.join(models_dir, f"{EXPERIMENT_NAME}_best.zip")
        if os.path.exists(best_model_path):
            save_model_config(best_model_path, model_config)
        print(f"\n‚úì –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {final_model_path}")
        
        # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤
        print("\n‚è≥ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –æ–±—É—á–µ–Ω–∏—è...")
        plot_path = f"RL3/{EXPERIMENT_NAME}_training.png"
        plot_training_results(logs_dir, plot_path)
        
        print("\n" + "=" * 60)
        print("–û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!")
        print("=" * 60)
        print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤:")
        print(f"  - –ú–æ–¥–µ–ª–∏: {models_dir}")
        print(f"  - –õ–æ–≥–∏: {logs_dir}")
        print(f"  - TensorBoard: {tensorboard_dir}")
        print(f"  - –ì—Ä–∞—Ñ–∏–∫: RL3/training_results.png")
        
        print("\n–î–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –ª–æ–≥–æ–≤ TensorBoard –≤—ã–ø–æ–ª–Ω–∏—Ç–µ:")
        print(f"  tensorboard --logdir={tensorboard_dir}")
        
        print("\n–î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ:")
        print("  python RL3/test_trained_model.py")
        
    except KeyboardInterrupt:
        print("\n‚ö† –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        interrupted_path = os.path.join(models_dir, f"{EXPERIMENT_NAME}_interrupted.zip")
        model.save(interrupted_path)
        save_model_config(interrupted_path, model_config)
        print("‚úì –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
    
    finally:
        env.close()
        eval_env.close()


if __name__ == "__main__":
    # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    import random
    random.seed(42)
    np.random.seed(42)
    torch.manual_seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(42)
    
    train()
