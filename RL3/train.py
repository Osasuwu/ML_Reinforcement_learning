"""
–°–∫—Ä–∏–ø—Ç –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∑–∞–¥–∞—á–∏ pick-and-place.

–í—Ö–æ–¥: —Ç–æ–ª—å–∫–æ –ø–∏–∫—Å–µ–ª–∏ (64x64 grayscale)
–ú–æ–¥–µ–ª—å: MobileNetV3-Small —Å –∑–∞–º–æ—Ä–æ–∑–∫–æ–π —Å–ª–æ—ë–≤
–¶–µ–ª—å: 1-2M —à–∞–≥–æ–≤ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —É—Å–ø–µ—Ö–∞
"""
import os
import json
import argparse
import numpy as np
import torch
from stable_baselines3 import PPO, SAC
from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv, VecMonitor
from stable_baselines3.common.callbacks import (
    CheckpointCallback, EvalCallback, CallbackList, BaseCallback
)
from stable_baselines3.common.monitor import Monitor

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ RL3 –¥–ª—è –∏–º–ø–æ—Ä—Ç–æ–≤
import sys
script_dir = os.path.dirname(os.path.abspath(__file__))
if script_dir not in sys.path:
    sys.path.insert(0, script_dir)

from robot_env import RobotEnv
from feature_extractor import (MobileNetExtractor, EfficientNetExtractor, 
                               SimpleCNNExtractor, MobileNetWithDepthExtractor)


def parse_args():
    parser = argparse.ArgumentParser(description='Train pick-and-place robot')
    parser.add_argument('--steps', type=int, default=10_000_000,
                       help='Max timesteps (default: 10M)')
    parser.add_argument('--target-success', type=float, default=80.0,
                       help='Target success rate %% to stop training (default: 80)')
    parser.add_argument('--patience', type=int, default=500_000,
                       help='Stop if no improvement for N steps (default: 500k)')
    parser.add_argument('--min-steps', type=int, default=1_000_000,
                       help='Start patience check after N steps (default: 1M)')
    parser.add_argument('--network', type=str, default='mobilenet',
                       choices=['mobilenet', 'efficientnet', 'cnn'],
                       help='Network architecture')
    parser.add_argument('--algo', type=str, default='ppo',
                       choices=['ppo', 'sac'],
                       help='RL algorithm')
    parser.add_argument('--camera', type=str, default='side',
                       choices=['side', 'side+depth', 'side+wrist'],
                       help='Camera mode (side+depth/wrist adds 8x8 secondary cam)')
    parser.add_argument('--freeze', type=int, default=8,
                       help='Number of layers to freeze in pretrained model')
    parser.add_argument('--n_envs', type=int, default=4,
                       help='Number of parallel environments')
    parser.add_argument('--image_size', type=int, default=64,
                       help='Image size (64 recommended)')
    parser.add_argument('--eval_freq', type=int, default=25000,
                       help='Evaluation frequency')
    parser.add_argument('--seed', type=int, default=42,
                       help='Random seed')
    parser.add_argument('--render', type=int, default=2,
                       help='Episodes to render at each checkpoint (0 = disable)')
    parser.add_argument('--curriculum', action='store_true',
                       help='Dynamic curriculum: fixed object until 30%% grasp rate, then random')
    return parser.parse_args()


class ProgressCallback(BaseCallback):
    """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
    def __init__(self, check_freq=10000, verbose=1):
        super().__init__(verbose)
        self.check_freq = check_freq
        self.best_success_rate = 0
        self.episode_successes = []
        
    def _on_step(self) -> bool:
        # –°–æ–±–∏—Ä–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± —É—Å–ø–µ—Ö–∞—Ö
        for info in self.locals.get('infos', []):
            if 'success' in info:
                self.episode_successes.append(info['success'])
        
        if self.n_calls % self.check_freq == 0 and len(self.model.ep_info_buffer) > 0:
            rewards = [ep['r'] for ep in self.model.ep_info_buffer]
            mean_reward = np.mean(rewards)
            
            # –£—Å–ø–µ—à–Ω–æ—Å—Ç—å –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —ç–ø–∏–∑–æ–¥—ã
            recent_successes = self.episode_successes[-100:] if self.episode_successes else []
            success_rate = np.mean(recent_successes) * 100 if recent_successes else 0
            
            marker = ""
            if success_rate > self.best_success_rate:
                self.best_success_rate = success_rate
                marker = " üéØ NEW BEST!"
            
            print(f"\nStep {self.n_calls:,}: "
                  f"Mean R = {mean_reward:.2f}, "
                  f"Success = {success_rate:.1f}%{marker}")
        
        return True


class SuccessRateCallback(BaseCallback):
    """–õ–æ–≥–∏—Ä—É–µ—Ç success rate –¥–ª—è TensorBoard"""
    def __init__(self, verbose=0):
        super().__init__(verbose)
        self.successes = []
        
    def _on_step(self) -> bool:
        for info in self.locals.get('infos', []):
            if 'success' in info:
                self.successes.append(float(info['success']))
        
        if len(self.successes) >= 100:
            success_rate = np.mean(self.successes[-100:])
            self.logger.record('rollout/success_rate', success_rate)
        
        return True
    
    def get_success_rate(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â–∏–π success rate"""
        if len(self.successes) >= 50:
            return np.mean(self.successes[-100:])
        return 0.0


class EarlyStoppingCallback(BaseCallback):
    """
    –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ target success rate 
    –∏–ª–∏ –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ —É–ª—É—á—à–µ–Ω–∏–π (stagnation).
    
    Patience –Ω–∞—á–∏–Ω–∞–µ—Ç —Å—á–∏—Ç–∞—Ç—å—Å—è —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ min_steps (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1M).
    –¢–∞–∫–∂–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—Ä—É—á–Ω—É—é –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ —Ñ–∞–π–ª STOP.
    """
    def __init__(self, success_callback, target_success=0.8, patience=500000,
                 min_steps=1_000_000, check_freq=10000, stop_file="STOP", verbose=1):
        super().__init__(verbose)
        self.success_callback = success_callback
        self.target_success = target_success  # 0.0-1.0
        self.patience = patience
        self.min_steps = min_steps  # Patience –∞–∫—Ç–∏–≤–∏—Ä—É–µ—Ç—Å—è –ø–æ—Å–ª–µ —ç—Ç–æ–≥–æ —à–∞–≥–∞
        self.check_freq = check_freq
        self.stop_file = stop_file
        
        self.best_success = 0.0
        self.steps_without_improvement = 0
        self.last_check_step = 0
        self.stop_reason = None
        self.patience_active = False
        
    def _on_step(self) -> bool:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –∫–∞–∂–¥—ã–µ check_freq —à–∞–≥–æ–≤
        if self.n_calls - self.last_check_step < self.check_freq:
            return True
        
        self.last_check_step = self.n_calls
        
        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–∞ —Ä—É—á–Ω–æ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
        script_dir = os.path.dirname(os.path.abspath(__file__))
        stop_path = os.path.join(script_dir, self.stop_file)
        if os.path.exists(stop_path):
            self.stop_reason = "manual_stop"
            print(f"\n{'='*60}")
            print(f"[STOP] Manual stop requested (STOP file found)")
            print(f"{'='*60}")
            # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª
            try:
                os.remove(stop_path)
            except:
                pass
            return False
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–π success rate
        current_success = self.success_callback.get_success_rate()
        
        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–∏
        if current_success >= self.target_success:
            self.stop_reason = "target_reached"
            print(f"\n{'='*60}")
            print(f"[SUCCESS] TARGET REACHED!")
            print(f"  Current success rate: {current_success*100:.1f}%")
            print(f"  Target: {self.target_success*100:.1f}%")
            print(f"  Steps: {self.num_timesteps:,}")
            print(f"{'='*60}")
            return False
        
        # 3. –ê–∫—Ç–∏–≤–∞—Ü–∏—è patience –ø–æ—Å–ª–µ min_steps
        if not self.patience_active and self.num_timesteps >= self.min_steps:
            self.patience_active = True
            self.best_success = current_success
            self.steps_without_improvement = 0
            if self.verbose:
                print(f"\n  [Patience activated at {self.num_timesteps:,} steps]")
                print(f"  Current best: {self.best_success*100:.1f}%")
        
        # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–ª—É—á—à–µ–Ω–∏—è (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ patience –∞–∫—Ç–∏–≤–µ–Ω)
        if self.patience_active:
            if current_success > self.best_success + 0.01:  # +1% improvement
                self.best_success = current_success
                self.steps_without_improvement = 0
                if self.verbose:
                    print(f"\n  [Improvement] Best success rate: {self.best_success*100:.1f}%")
            else:
                self.steps_without_improvement += self.check_freq
            
            # 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ stagnation
            if self.steps_without_improvement >= self.patience:
                self.stop_reason = "stagnation"
                print(f"\n{'='*60}")
                print(f"[STOP] Training stagnated!")
                print(f"  No improvement for {self.patience:,} steps (after {self.min_steps:,})")
                print(f"  Best success rate: {self.best_success*100:.1f}%")
                print(f"  Current: {current_success*100:.1f}%")
                print(f"{'='*60}")
                return False
        
        return True
    
    def _on_training_end(self):
        if self.stop_reason:
            print(f"\nTraining ended: {self.stop_reason}")


class PeriodicRenderCallback(BaseCallback):
    """
    –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç —Ä–µ–Ω–¥–µ—Ä-—Ç–µ—Å—Ç.
    Checkpoints: –∫–∞–∂–¥—ã–µ 10k –¥–æ 100k, –∫–∞–∂–¥—ã–µ 50k –¥–æ 500k, –∫–∞–∂–¥—ã–µ 200k –ø–æ—Å–ª–µ.
    """
    def __init__(self, models_dir, exp_name, env_kwargs, total_steps, 
                 render_episodes=2, verbose=1):
        super().__init__(verbose)
        self.models_dir = models_dir
        self.exp_name = exp_name
        self.env_kwargs = env_kwargs
        self.total_steps = total_steps
        self.render_episodes = render_episodes
        self.render_process = None
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ checkpoints –∑–∞—Ä–∞–Ω–µ–µ
        self.checkpoints = self._generate_checkpoints(total_steps)
        self.next_checkpoint_idx = 0
        
    def _generate_checkpoints(self, total_steps):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ checkpoints"""
        checkpoints = []
        
        # –ö–∞–∂–¥—ã–µ 10k –¥–æ 100k
        for step in range(10000, min(100001, total_steps + 1), 10000):
            checkpoints.append(step)
        
        # –ö–∞–∂–¥—ã–µ 50k –æ—Ç 100k –¥–æ 500k  
        for step in range(150000, min(500001, total_steps + 1), 50000):
            checkpoints.append(step)
        
        # –ö–∞–∂–¥—ã–µ 200k –ø–æ—Å–ª–µ 500k
        for step in range(600000, total_steps + 1, 200000):
            checkpoints.append(step)
        
        return sorted(set(checkpoints))
    
    def _on_step(self) -> bool:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç–∏–≥–ª–∏ –ª–∏ —Å–ª–µ–¥—É—é—â–µ–≥–æ checkpoint
        if self.next_checkpoint_idx >= len(self.checkpoints):
            return True
            
        next_checkpoint = self.checkpoints[self.next_checkpoint_idx]
        
        if self.num_timesteps >= next_checkpoint:
            self.next_checkpoint_idx += 1
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å (–∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å)
            save_path = os.path.abspath(os.path.join(
                self.models_dir, 
                f"{self.exp_name}_{next_checkpoint//1000}k.zip"
            ))
            self.model.save(save_path)
            
            if self.verbose:
                print(f"\n[CHECKPOINT {next_checkpoint//1000}k]")
                print(f"  Model saved: {save_path}")
                print(f"  Starting render test ({self.render_episodes} episodes)...")
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º —Ä–µ–Ω–¥–µ—Ä –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ
            self._start_render_test(save_path)
        
        return True
    
    def _start_render_test(self, model_path):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç —Ç–µ—Å—Ç –º–æ–¥–µ–ª–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ —Å GUI –æ–∫–Ω–æ–º"""
        import subprocess
        import sys
        
        # –ó–∞–≤–µ—Ä—à–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π –ø—Ä–æ—Ü–µ—Å—Å –µ—Å–ª–∏ –µ—â—ë —Ä–∞–±–æ—Ç–∞–µ—Ç
        if self.render_process is not None:
            try:
                self.render_process.terminate()
                self.render_process.wait(timeout=2)
            except:
                pass
        
        # –ü–æ–ª—É—á–∞–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å –∫ test.py
        script_dir = os.path.dirname(os.path.abspath(__file__))
        test_script = os.path.join(script_dir, "test.py")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º PowerShell Start-Process –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤–æ–≥–æ –æ–∫–Ω–∞
        ps_cmd = f'Start-Process -FilePath "{sys.executable}" -ArgumentList \'"{test_script}" "{model_path}" --episodes {self.render_episodes} --gui\''
        
        try:
            self.render_process = subprocess.Popen(
                ["powershell", "-Command", ps_cmd],
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL
            )
            if self.verbose:
                print(f"  [OK] Render window opened")
        except Exception as e:
            if self.verbose:
                print(f"  [!] Failed to start render: {e}")
    
    def _on_training_end(self):
        """–ñ–¥–µ–º –ø–æ–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–∞–∫—Ä–æ–µ—Ç –æ–∫–Ω–æ —Ä–µ–Ω–¥–µ—Ä–∞"""
        if self.render_process is not None:
            try:
                # –ñ–¥–µ–º –ø–æ–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å –∑–∞–≤–µ—Ä—à–∏—Ç—Å—è (–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–∞–∫—Ä–æ–µ—Ç –æ–∫–Ω–æ)
                self.render_process.wait()
            except:
                pass


def make_env(rank, use_gui=False, **env_kwargs):
    def _init():
        env = RobotEnv(use_gui=use_gui, **env_kwargs)
        env = Monitor(env)
        return env
    return _init


def train(args):
    print("=" * 60)
    print("PICK-AND-PLACE ROBOT TRAINING")
    print("=" * 60)
    
    # Experiment name
    exp_name = f"pickplace_{args.network}_{args.camera}_{args.algo}_{args.steps//1000}k"
    
    # Directories (–∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –ø—É—Ç–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ RL3)
    rl3_dir = os.path.dirname(os.path.abspath(__file__))
    models_dir = os.path.join(rl3_dir, "models")
    logs_dir = os.path.join(rl3_dir, "logs", exp_name)
    tensorboard_dir = os.path.join(rl3_dir, "tensorboard", exp_name)
    
    os.makedirs(models_dir, exist_ok=True)
    os.makedirs(logs_dir, exist_ok=True)
    os.makedirs(tensorboard_dir, exist_ok=True)
    
    # Environment kwargs
    env_kwargs = {
        'image_size': args.image_size,
        'frame_stack': 4,
        'camera_mode': args.camera,
        'max_steps': 200,
        'curriculum': args.curriculum,  # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π curriculum
        'curriculum_threshold': 0.3,  # 30% –∫–∞—Å–∞–Ω–∏–π –¥–ª—è –ø–µ—Ä–µ—Ö–æ–¥–∞ –Ω–∞ —Ä–∞–Ω–¥–æ–º
    }
    
    if args.curriculum:
        print("[CURRICULUM] Dynamic curriculum enabled!")
        print("  - Object position: FIXED until 30% grasp rate")
        print("  - Goal position: always RANDOM")
    
    # Config
    config = {
        "experiment_name": exp_name,
        "network": args.network,
        "algorithm": args.algo,
        "camera_mode": args.camera,
        "image_size": args.image_size,
        "freeze_layers": args.freeze,
        "total_timesteps": args.steps,
        "target_success": args.target_success,
        "patience": args.patience,
        "n_envs": args.n_envs,
        "seed": args.seed
    }
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥ —Å—Ä–∞–∑—É –≤ –Ω–∞—á–∞–ª–µ (–æ–¥–∏–Ω –¥–ª—è –≤—Å–µ—Ö checkpoints)
    config_path = os.path.join(models_dir, f"{exp_name}_config.json")
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=2)
    print(f"[OK] Config saved: {config_path}")
    
    print(f"\nExperiment: {exp_name}")
    print(f"\nSettings:")
    print(f"  - Network: {args.network}")
    print(f"  - Algorithm: {args.algo}")
    print(f"  - Camera: {args.camera}")
    print(f"  - Image size: {args.image_size}x{args.image_size}")
    print(f"  - Freeze layers: {args.freeze}")
    print(f"  - Max steps: {args.steps:,}")
    print(f"  - Target success: {args.target_success}%")
    print(f"  - Patience: {args.patience:,} steps")
    print(f"  - Parallel envs: {args.n_envs}")
    if args.render > 0:
        print(f"  - Periodic render: {args.render} episodes at checkpoints")
    
    # Device
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"  - Device: {device}")
    if device == "cuda":
        print(f"    GPU: {torch.cuda.get_device_name(0)}")
    
    # Seeds
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    
    # Create environments
    print("\nCreating environments...")
    
    if args.n_envs > 1:
        env = SubprocVecEnv([make_env(i, use_gui=False, **env_kwargs) for i in range(args.n_envs)])
    else:
        env = DummyVecEnv([make_env(0, use_gui=False, **env_kwargs)])
    env = VecMonitor(env, logs_dir)
    
    eval_env = DummyVecEnv([make_env(0, use_gui=False, **env_kwargs)])
    eval_env = VecMonitor(eval_env, logs_dir)
    
    print("[OK] Environments created")
    
    # Select feature extractor
    # –î–ª—è side+depth –∏ side+wrist –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π extractor —Å Dict observation
    if args.camera in ("side+depth", "side+wrist"):
        extractor_class = MobileNetWithDepthExtractor
        extractor_kwargs = {"features_dim": 256, "freeze_layers": args.freeze}
        policy_type = "MultiInputPolicy"  # –î–ª—è Dict observation space
    else:
        policy_type = "CnnPolicy"
        if args.network == "mobilenet":
            extractor_class = MobileNetExtractor
            extractor_kwargs = {"features_dim": 256, "freeze_layers": args.freeze}
        elif args.network == "efficientnet":
            extractor_class = EfficientNetExtractor
            extractor_kwargs = {"features_dim": 256, "freeze_layers": args.freeze}
        else:
            extractor_class = SimpleCNNExtractor
            extractor_kwargs = {"features_dim": 256}
    
    policy_kwargs = dict(
        features_extractor_class=extractor_class,
        features_extractor_kwargs=extractor_kwargs,
        net_arch=dict(pi=[256, 128], vf=[256, 128])
    )
    
    # Create model
    print(f"\nCreating {args.algo.upper()} model...")
    
    if args.algo == "ppo":
        model = PPO(
            policy_type,
            env,
            policy_kwargs=policy_kwargs,
            learning_rate=3e-4,
            n_steps=2048,
            batch_size=256,
            n_epochs=10,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            ent_coef=0.01,
            max_grad_norm=0.5,
            verbose=1,
            tensorboard_log=tensorboard_dir,
            device=device,
            seed=args.seed
        )
    else:  # SAC
        model = SAC(
            policy_type,
            env,
            policy_kwargs=policy_kwargs,
            learning_rate=3e-4,
            buffer_size=100000,
            learning_starts=1000,
            batch_size=256,
            tau=0.005,
            gamma=0.99,
            train_freq=1,
            gradient_steps=1,
            verbose=1,
            tensorboard_log=tensorboard_dir,
            device=device,
            seed=args.seed
        )
    
    print("[OK] Model created")
    
    # Callbacks
    eval_cb = EvalCallback(
        eval_env,
        best_model_save_path=models_dir,
        log_path=logs_dir,
        eval_freq=args.eval_freq,
        n_eval_episodes=10,
        deterministic=True,
        render=False
    )
    
    progress_cb = ProgressCallback(check_freq=10000)
    success_cb = SuccessRateCallback()
    
    # Early stopping callback
    early_stop_cb = EarlyStoppingCallback(
        success_callback=success_cb,
        target_success=args.target_success / 100.0,  # Convert % to ratio
        patience=args.patience,
        min_steps=args.min_steps,  # Patience –∞–∫—Ç–∏–≤–∏—Ä—É–µ—Ç—Å—è –ø–æ—Å–ª–µ —ç—Ç–æ–≥–æ —à–∞–≥–∞
        check_freq=10000
    )
    
    callbacks_list = [eval_cb, progress_cb, success_cb, early_stop_cb]
    
    # –î–æ–±–∞–≤–ª—è–µ–º PeriodicRenderCallback –µ—Å–ª–∏ –Ω—É–∂–µ–Ω —Ä–µ–Ω–¥–µ—Ä
    if args.render > 0:
        render_cb = PeriodicRenderCallback(
            models_dir=models_dir,
            exp_name=exp_name,
            env_kwargs=env_kwargs,
            total_steps=args.steps,
            render_episodes=args.render
        )
        callbacks_list.append(render_cb)
    
    callbacks = CallbackList(callbacks_list)
    
    # Training
    print("\n" + "=" * 60)
    print("STARTING TRAINING")
    print(f"  Target success rate: {args.target_success}%")
    print(f"  Max steps: {args.steps:,}")
    print(f"  Early stop patience: {args.patience:,} steps")
    print(f"  Manual stop: create 'RL3/STOP' file")
    if args.render > 0:
        print(f"  Periodic render: {args.render} episodes at checkpoints")
        print(f"  Save intervals: 10k -> 50k -> 200k steps")
    print("=" * 60)
    print(f"TensorBoard: tensorboard --logdir={tensorboard_dir}")
    print("=" * 60 + "\n")
    
    try:
        model.learn(
            total_timesteps=args.steps,
            callback=callbacks,
            progress_bar=True
        )
        
        # Save final model
        final_path = os.path.join(models_dir, f"{exp_name}_final.zip")
        model.save(final_path)
        
        # Save config
        config_path = final_path.replace('.zip', '_config.json')
        with open(config_path, 'w') as f:
            json.dump(config, f, indent=2)
        
        print(f"\n[OK] Final model saved: {final_path}")
        
        # Rename best_model
        best_src = os.path.join(models_dir, "best_model.zip")
        best_dst = os.path.join(models_dir, f"{exp_name}_best.zip")
        if os.path.exists(best_src):
            if os.path.exists(best_dst):
                os.remove(best_dst)
            os.rename(best_src, best_dst)
            
            config_dst = best_dst.replace('.zip', '_config.json')
            with open(config_dst, 'w') as f:
                json.dump(config, f, indent=2)
            
            print(f"[OK] Best model saved: {best_dst}")
        
        print("\n" + "=" * 60)
        print("TRAINING COMPLETED!")
        print("=" * 60)
        
    except KeyboardInterrupt:
        print("\n[!] Training interrupted")
        interrupted_path = os.path.join(models_dir, f"{exp_name}_interrupted.zip")
        model.save(interrupted_path)
        print(f"[OK] Model saved: {interrupted_path}")
    
    finally:
        env.close()
        eval_env.close()


if __name__ == "__main__":
    args = parse_args()
    train(args)
