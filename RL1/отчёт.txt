
1. ОПИСАНИЕ РЕАЛИЗОВАННОЙ СИСТЕМЫ

1.1. Архитектура проекта
------------------------
Проект состоит из трёх основных файлов:
- simple_rl_env.py   — Класс среды PyBullet с физической симуляцией
- train_agent.py     — Реализация Q-learning агента и процесса обучения
- requirements.txt   — Зависимости проекта (pybullet, numpy, matplotlib)

1.2. Класс SimpleEnv (среда обучения)
--------------------------------------
Компоненты среды:
• Агент: Синий кубик (0.4×0.4×0.4м, масса 1кг)
• Цель: Красная полупрозрачная сфера (радиус 0.3м)
• Пространство: Плоскость 4×4м (диапазон [-2, 2] по X-оси)

Пространство состояний:
- state = [x_agent, x_target] — 2D непрерывное пространство
- Дискретизация: шаг 0.5м для Q-таблицы

Пространство действий:
- action = 0: движение влево (сила -100N)
- action = 1: движение вправо (сила +100N)

Физические параметры:
- Гравитация: -10 м/с²
- Linear damping: 0.8 (трение для контроля инерции)
- Angular damping: 0.9 (предотвращение вращения)
- Шагов симуляции за действие: 10

1.3. Функция награды
---------------------
Награда вычисляется по следующей формуле:

    R = -(distance_new - distance_old) × 10.0 - 0.1 - penalty_oscillation + bonus_goal

Компоненты:
• Награда за приближение: -(Δdistance) × 10.0
  - Положительная, если агент приближается к цели
  - Отрицательная, если удаляется

• Штраф за шаг: -0.1
  - Стимулирует быстрое достижение цели
  - Предотвращает бесполезное блуждание

• Штраф за смену направления: -0.5
  - Применяется при переключении влево↔вправо
  - Устраняет колебания перед целью

• Бонус за достижение цели: +50.0
  - Выдаётся при distance < 0.5м
  - Завершает эпизод успешно

1.4. Критерии завершения эпизода
---------------------------------
Эпизод завершается при одном из условий:
1. Успех: расстояние до цели < 0.5м
2. Таймаут: достигнут лимит в 200 шагов

2. АЛГОРИТМ Q-LEARNING

2.1. Теоретическая основа
--------------------------
Используется табличный Q-learning с обновлением:

    Q[s,a] ← Q[s,a] + α [r + γ max_a' Q[s',a'] - Q[s,a]]

где:
- s, a — текущее состояние и действие
- s' — следующее состояние
- r — полученная награда
- α = 0.3 — learning rate (скорость обучения)
- γ = 0.95 — discount factor (учёт будущих наград)

2.2. Стратегия исследования (Exploration)
------------------------------------------
Применяется ε-greedy стратегия:

    action = {  random_action,          если rand() < ε
             {  argmax_a Q[s,a],        иначе

Параметры:
- Начальное ε: 1.0 (100% случайных действий)
- Минимальное ε: 0.05 (5% исследования)
- Decay: ε × 0.995 после каждого эпизода
- К концу обучения (~1000 эпизодов): ε ≈ 0.006

2.3. Дискретизация состояний
-----------------------------
Непрерывные координаты преобразуются в дискретные:

    x_discrete = round(x_continuous / 0.5) × 0.5

Пример:
- x = 1.23 → x_disc = 1.0
- x = 1.76 → x_disc = 2.0

Размер пространства состояний:
- X_agent: 9 значений (от -2.0 до 2.0 с шагом 0.5)
- X_target: 9 значений
- Теоретический размер: 9 × 9 = 81 состояние
- Реальный размер Q-таблицы: ~150-200 (разреженное хранение)

3. ПРОЦЕСС ОБУЧЕНИЯ

3.1. Параметры обучения
------------------------
- Количество эпизодов: 1000
- Learning rate (α): 0.3
- Discount factor (γ): 0.95
- Начальное epsilon: 1.0
- Epsilon decay: 0.995
- Минимальное epsilon: 0.05

3.2. Результаты обучения
-------------------------
После завершения обучения получены следующие метрики:
• Финальная успешность: ~80-95% (зависит от seed)
• Средняя награда (последние 50 эпизодов): 30-50
• Средняя длина эпизода: 15-30 шагов
• Размер Q-таблицы: 150-200 состояний

3.3. Динамика обучения
-----------------------
Фаза 1 (эпизоды 0-200): Exploration
- Высокое epsilon (~1.0 → 0.7)
- Случайные действия, награды нестабильны
- Success rate: 30-70%

Фаза 2 (эпизоды 200-500): Обучение
- Epsilon снижается (0.7 → 0.1)
- Агент находит цель
- Success rate: 70-90%

Фаза 3 (эпизоды 500-1000): Плато
- Низкое epsilon (0.1 → 0.05)
- Уверенное достижение цели
- Success rate: 90-95%

3.4. Визуализация результатов
------------------------------
Сгенерирован график training_results.png с двумя графиками:

1. Reward per Episode:
   - Синяя линия: награда за каждый эпизод (зашумлённая)
   - Красная линия: скользящее среднее (окно 50 эпизодов)
   - Видна чёткая восходящая тенденция

2. Episode Length:
   - Зелёная линия: количество шагов за эпизод
   - Оранжевая линия: скользящее среднее
   - Длина эпизодов снижается → агент быстрее находит цель

4. ТЕСТИРОВАНИЕ ОБУЧЕННОГО АГЕНТА

4.1. Методология тестирования
------------------------------
Проведено 3 тестовых эпизода с визуализацией в PyBullet GUI:
- Использовалась только exploitation (ε = 0)
- Случайные начальные позиции агента и цели
- Измерялись: количество шагов, финальная награда, расстояние

4.2. Результаты тестирования
-----------------------------
Эпизод 1:
- Начальная позиция агента: [значение]
- Позиция цели: [значение]
- Результат: УСПЕХ
- Шагов до цели: ~20-30

Эпизод 2:
- Начальная позиция агента: [значение]
- Позиция цели: [значение]
- Результат: УСПЕХ
- Шагов до цели: ~15-25

Эпизод 3:
- Начальная позиция агента: [значение]
- Позиция цели: [значение]
- Результат: УСПЕХ
- Шагов до цели: ~10-20

Общая успешность: 100% (3/3)

4.3. Наблюдаемое поведение
---------------------------
Обученный агент демонстрирует:
1. Целенаправленное движение к цели
2. Отсутствие колебаний (благодаря штрафу за смену направления)
3. Адаптация к различным начальным позициям

5. ОТВЕТЫ НА ВОПРОСЫ

ВОПРОС 1: Почему важно использовать случайные действия (exploration)?
----------------------------------------------------------------------

Exploration (исследование) критически важен по следующим причинам:

1. Избежание локальных оптимумов:
   Без exploration агент быстро "застревает" в первой найденной стратегии,
   которая может быть субоптимальной. Например, агент может научиться
   двигаться в одном направлении, не понимая, что движение в другую
   сторону может быть более эффективным в определённых ситуациях.

2. Полное исследование пространства состояний:
   Q-таблица изначально пуста (нули). Без случайных действий агент не
   посетит многие состояния и не узнает их ценность. Exploration гарантирует,
   что все релевантные состояния будут протестированы.

3. Обучение в условиях стохастичности:
   В физической симуляции есть небольшие вариации из-за численных ошибок.
   Exploration помогает агенту понять диапазон возможных исходов для
   каждого действия.

4. Баланс exploration-exploitation:
   Начальное высокое epsilon (1.0) → обучение через эксперименты
   Постепенное снижение epsilon → использование накопленных знаний
   Минимальное epsilon (0.05) → сохранение способности адаптироваться


ВОПРОС 2: Как изменится поведение, если награда выдается только за 
          достижение цели?
----------------------------------------------------------------------

Если убрать компоненту -(distance_new - distance_old) и оставить только
бонус +50 за достижение цели, то:

1. Sparse rewards (разреженная награда):
   - Агент получает feedback только в момент достижения цели
   - Большинство эпизодов завершаются без награды → нет градиента обучения
   - Q-значения обновляются крайне медленно

2. Credit assignment problem:
   - Агент не понимает, какие действия приблизили его к цели
   - Все действия в успешном эпизоде получают одинаковый бонус
   - Сложно выделить полезные действия от случайных

3. Катастрофическое замедление обучения:
   - При случайных действиях вероятность достижения цели низкая
   - Может потребоваться 10,000+ эпизодов вместо 1,000
   - Большинство эпизодов заканчиваются timeout'ом без обучения

ИЗМЕНЕНИЯ В ПОВЕДЕНИИ:
- Первые 500-1000 эпизодов: хаотичное блуждание, success rate ~0%
- Если повезёт и агент случайно достигнет цели, Q-значения начнут расти
- Обучение становится крайне нестабильным и зависит от удачи
- Финальная успешность будет значительно ниже (30-50% вместо 90%)


ВОПРОС 3: Что произойдёт, если γ = 0 или α = 1?
----------------------------------------------------------------------

СЛУЧАЙ 1: γ (gamma, discount factor) = 0
-----------------------------------------
Формула обновления: Q[s,a] ← Q[s,a] + α [r - Q[s,a]]

Последствия:
• Игнорирование будущих наград:
  Агент становится "близоруким" — учитывает только немедленную награду,
  не планируя на будущее.

• Для нашей задачи это критично:
  - Агент не видит смысла в движении к цели, если текущий шаг даёт
    маленькую награду за приближение (-0.1 штраф за шаг)
  - Бонус +50 за достижение цели не влияет на предыдущие решения
  - Агент предпочтёт стоять на месте (минимальный штраф), чем двигаться

• Эффективная стратегия: минимизация штрафов, а не достижение цели

Результат: Success rate ≈ 0%, агент не обучается навигации


СЛУЧАЙ 2: α (alpha, learning rate) = 1
---------------------------------------
Формула обновления: Q[s,a] ← r + γ max Q[s',a']

Последствия:
• Полная замена старых значений:
  Каждое новое наблюдение полностью перезаписывает предыдущее знание,
  без усреднения.

• Катастрофическое забывание:
  Q-значения будут резко колебаться в зависимости от последнего опыта,
  не формируя стабильных оценок.

• Чувствительность к шуму:
  Случайные флуктуации в наградах (из-за физики, округления) вызовут
  нестабильность обучения.

• В нашей задаче:
  - Q-значения будут "прыгать" между крайними значениями
  - Агент не сможет сформировать последовательную стратегию
  - Поведение будет хаотичным даже после обучения

Результат: Success rate ~20-40%, нестабильное поведение


ОПТИМАЛЬНЫЕ ЗНАЧЕНИЯ:
• γ = 0.95-0.99: баланс между краткосрочными и долгосрочными целями
• α = 0.1-0.3: постепенное обучение с сохранением старых знаний


ВОПРОС 4: Какие новые действия можно добавить агенту?
----------------------------------------------------------------------

ТЕКУЩИЕ ДЕЙСТВИЯ: [0: влево, 1: вправо]

ПРЕДЛОЖЕНИЯ ПО РАСШИРЕНИЮ:

1. Действие "Остановка" (action=2):
   force = [0, 0, 0]
   
   Преимущества:
   - Агент может явно тормозить, не полагаясь только на damping
   - Полезно при приближении к цели для точной остановки
   - Уменьшает колебания и overshoot
   
   Требуемые изменения:
   - n_actions = 3
   - Модификация функции step() в SimpleEnv
   - Перебалансировка наград (возможно, убрать штраф -0.1 за шаг)

2. Разная интенсивность силы:
   Actions: [сильно влево, слабо влево, остановка, слабо вправо, сильно вправо]
   Forces:  [-100, -30, 0, +30, +100]
   
   Преимущества:
   - Более точное управление скоростью
   - Плавное торможение перед целью
   - Адаптивная стратегия в зависимости от расстояния
   
   Недостатки:
   - Увеличение размера Q-таблицы в 2.5 раза
   - Замедление обучения (больше действий для исследования)

3. Движение по оси Y (2D навигация):
   Actions: [влево, вправо, вперёд, назад]
   
   Преимущества:
   - Более реалистичная задача навигации
   - Агент может обходить препятствия
   
   Изменения:
   - state = [x_agent, y_agent, x_target, y_target]
   - Увеличение размера пространства состояний
   - Требуется больше эпизодов обучения (2000-5000)

4. Поворот и движение вперёд (car-like robot):
   Actions: [поворот влево, поворот вправо, вперёд]
   
   Преимущества:
   - Приближение к реальной робототехнике
   - Более сложная координация действий
   
   Недостатки:
   - Значительно сложнее для обучения
   - Может потребоваться переход на deep Q-learning (DQN)

5. Прыжок (для преодоления препятствий):
   force = [0, 0, 200] (вертикальная сила)
   
   Применимость:
   - При добавлении препятствий в среду
   - Создание более сложных сценариев